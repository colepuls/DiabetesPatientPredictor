{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"/Users/colepuls/CS/MUHackathon2025/Raw_data/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic stats\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values or weird entries\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count duplicate how many duplicate rows exist\n",
    "num_dups = data.duplicated().sum()\n",
    "num_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any dups\n",
    "if num_dups > 0:\n",
    "    data.drop_duplicates(inpalce=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/43/2x89vvws2g56p_wbgrnwxvlm0000gn/T/ipykernel_93427/3010604706.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[c].replace(0, np.nan, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Glucose          0\n",
       "BloodPressure    0\n",
       "SkinThickness    0\n",
       "Insulin          0\n",
       "BMI              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List columns where zeros are likely invalid or indicate missing data\n",
    "import numpy as np\n",
    "\n",
    "cols_to_clean = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
    "\n",
    "# Replace 0 with nan, indicates missing value\n",
    "for c in cols_to_clean:\n",
    "    data[c].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Check how many nans are in columns\n",
    "data[cols_to_clean].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values, fill with column means\n",
    "data[cols_to_clean] = data[cols_to_clean].fillna(data[cols_to_clean].mean())\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Split targets\n",
    "x = data.drop(columns=[\"Outcome\"]).values\n",
    "y = data[\"Outcome\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into tensors\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long) # for classification labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and std for each feature (column)\n",
    "mean = x_tensor.mean(dim=0)\n",
    "std = x_tensor.std(dim=0)\n",
    "\n",
    "std[std == 0] = 1e-7 # avoid division by zero\n",
    "\n",
    "# Normalize!!!\n",
    "x_tensor = (x_tensor - mean) / std # x tensor is now normalized column wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# initialize dataset    \n",
    "dataset = DiabetesDataset(x_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset) # number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_features = dataset[0]\n",
    "sample_labels = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# 80/20 split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# test\n",
    "for batch_features, batch_labels in train_loader:\n",
    "    print(batch_features.shape, batch_labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute stats after normalization\n",
    "check_mean = torch.mean(torch.vstack([dataset[i][0] for i in range(len(dataset))]), dim=0)\n",
    "check_std = torch.std(torch.vstack([dataset[i][0] for i in range(len(dataset))]), dim=0)\n",
    "\n",
    "check_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=16, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = SimpleNet(input_dim=x.shape[1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Train Loss: 0.5654 | Val Loss: 0.5041 | Val Acc: 77.92%\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for val_features, val_labels in val_loader:\n",
    "        val_outputs = model(val_features)\n",
    "        loss = criterion(val_outputs, val_labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(val_outputs, dim=1)\n",
    "        correct += (predicted == val_labels).sum().item()\n",
    "        total += val_labels.size(0)\n",
    "avg_val_loss = val_loss / len(val_loader)\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "      f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "      f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "      f\"Val Acc: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
